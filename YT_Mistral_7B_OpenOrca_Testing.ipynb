{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spectralgo/notebooks/blob/main/YT_Mistral_7B_OpenOrca_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install langchain"
      ],
      "metadata": {
        "id": "TOyVaq6r3oVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac257e0-ca61-4465-d8d0-57a8e04ca0fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca"
      ],
      "metadata": {
        "id": "jxxZXFiKtgwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "0e6cf152-98d4-4c60-973d-594e202ca2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  5 17:05:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Open-Orca/Mistral-7B-OpenOrca\n",
        "\n"
      ],
      "metadata": {
        "id": "H0shki19igLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "id": "NhvFmDhU1RQW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\",\n",
        "                                             torch_dtype=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\",\n",
        "                                          torch_dtype=\"auto\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "2813ee23de994e4793edc7ce42fb03c0",
            "3b48dee6366d47cc97e83e94c4dc24e8",
            "c6ce5af9770147729d95da634891188f",
            "da146134fced4f56a4432c4eb8bddd25",
            "9a7667ed475b4b2894ddaefab86fe988",
            "f7cba67739fa4d7b9ca5cc28f4a44339",
            "848c17136d6b4b359a968570aa307285",
            "72fb939b7dc140fcac04f4ad213c1e73",
            "dfccbda1867f451ebd414fafd1f5ef34",
            "818cca127327458ea7859d7bd700e61a",
            "b3b7b67fe75d4d528938116dd8423f77"
          ]
        },
        "id": "WsYot3Rz1NVf",
        "outputId": "c27cd927-b424-43a3-ffa3-adb1c7818fc4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2813ee23de994e4793edc7ce42fb03c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Format\n",
        "```\n",
        "<|im_start|>system\n",
        "You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "How are you?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I am doing well!<|im_end|>\n",
        "<|im_start|>user\n",
        "Please tell me about how mistral winds have attracted super-orcas.<|im_end|>\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "w9ykaBB_FF05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "import re\n",
        "import json\n",
        "\n",
        "def extract_translation(markdown_text):\n",
        "    # Extract text inside triple backticks\n",
        "    matches = re.findall(r\"```json\\s*([\\s\\S]*?)\\s*```\", markdown_text)\n",
        "    count = 0\n",
        "    for match in matches:\n",
        "        try:\n",
        "            # Parse the extracted text as JSON\n",
        "            parsed_json = json.loads(match)\n",
        "            # Extract value of \"translation\" key\n",
        "            if \"translation\" in parsed_json:\n",
        "                count += 1\n",
        "                if count == 2:  # If it's the second occurrence\n",
        "                    return parsed_json[\"translation\"]\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "text_to_translate = \"\"\"\n",
        " l'@1620647767959000 fonctionne en \"mode autonome\" et s'approche d'un mur (@1620647770151000).\\n\\nQuand l'@1620647767959000 arrive au bout de l'@1620647770151000, un piéton @1620647778714000 apparaît soudainement.\\n\\nL'@1620647767959000 évite le piéton lorsqu'il le détecte et @1620647778714000 continue de marcher tout droit.\n",
        "\"\"\"\n",
        "system_instructions = f\"\"\"<|im_start|> system:\\n You are a translator assistant.\n",
        "Do not add any explanations. Do not replace the placeholders like '@1620647767959000'.\n",
        "Respond with a json object like this: \\n\n",
        "{{\"translation\":  \"This is an example of a string respecting the json format\"}} \\n\n",
        "Example: \\n\n",
        " user:\\n\n",
        "Translate the text\n",
        "that is delimited by triple backticks\n",
        "into an idiomatic American English.\n",
        "text: ```Bonjour, je suis un @1620647767959000 dans une \"maison\"```\n",
        "\n",
        " returns:\\n\n",
        "```json\n",
        "{{\"translation\":  \"Hello, I'm a @1620647767959000 in a 'house'\"}}\n",
        "``` \\n\n",
        "<|im_end|>\\n\"\"\"\n",
        "\n",
        "text = f\"\"\"{system_instructions} <|im_start|> user:\\n Translate the text\n",
        "that is delimited by triple backticks\n",
        "into an idiomatic American English.\n",
        "text: ```{text_to_translate}```\\n <|im_end|>\"\"\"\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = encodeds.to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm3FAkWVVmJV",
        "outputId": "2cb43655-3ef5-4b3e-dfa3-8d4623c19ac9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The @1620647767959000 works in 'self-mode' and approaches a wall (@1620647770151000). When the @1620647767959000 reaches the @1620647770151000, a pedestrian (@1620647778714000) suddenly appears. The @1620647767959000 avoids the pedestrian when detected and @1620647778714000 continues to walk straight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_translation(decoded[0]))"
      ],
      "metadata": {
        "id": "HwN798Iw3bwe",
        "outputId": "8767d2d1-5d30-4711-a10e-169f70329c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The @1620647767959000 works in 'self-mode' and approaches a wall (@1620647770151000). When the @1620647767959000 reaches the @1620647770151000, a pedestrian (@1620647778714000) suddenly appears. The @1620647767959000 avoids the pedestrian when detected and @1620647778714000 continues to walk straight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "ocdzRuhhUTjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYUiuyCWFE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = f\"\"\"<|im_start|> system\\n{system_prompt}<|im_end|>\"\"\"\n",
        "    else:\n",
        "        system_prompt = \"\"\n",
        "    prompt = f\"\"\"<|im_start|> user\\n{input_text}<|im_end|>\"\"\"\n",
        "    final_prompt = system_prompt + prompt\n",
        "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    model_inputs = encodeds.to(device)\n",
        "    model.to(device)\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_length=max_length,\n",
        "                             temperature=0.1,\n",
        "                             pad_token_id = 3200,\n",
        "                             do_sample=True)\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "    # text = text[len(final_prompt):] if text.startswith(final_prompt) else text\n",
        "    text = text.replace(final_prompt, '', 1)\n",
        "    wrapped_text = wrap_text(text)\n",
        "    print(wrapped_text)"
      ],
      "metadata": {
        "id": "E4mjX62HFPIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CodeGen"
      ],
      "metadata": {
        "id": "mlrwEjmvLZ1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', system_prompt=\"You are a genius python coder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0M8bf36Cj4Z",
        "outputId": "646aef6d-8c8b-4851-c349-c0318724a103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   for i in range(2, n):\n",
            "       if is_prime(i):\n",
            "           print(i)\n",
            "\n",
            "def is_prime(n):\n",
            "   \"\"\"\n",
            "   Check if a number is prime\n",
            "   \"\"\"\n",
            "   if n < 2:\n",
            "       return False\n",
            "   for i in range(2, n):\n",
            "       if n % i == 0:\n",
            "           return False\n",
            "   return True\n",
            "\n",
            "print_prime(10)\n",
            "```\n",
            "```\n",
            "\n",
            "2\n",
            "3\n",
            "5\n",
            "7\n",
            "```\n",
            "\n",
            "This code prints all prime numbers between 1 and 10.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('''```python\n",
        "def detect_prime(n):\n",
        "   \"\"\"\n",
        "   detect if a number is a prime number or not. return True or False\n",
        "   \"\"\"''', system_prompt=\"You are a genius python coder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGJbmCoYLfYR",
        "outputId": "6939e6e0-4c33-4b04-8f65-4305cf613a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   if n <= 1:\n",
            "       return False\n",
            "\n",
            "   for i in range(2, n):\n",
            "       if n % i == 0:\n",
            "           return False\n",
            "\n",
            "   return True\n",
            "\n",
            "def main():\n",
            "    n = int(input(\"Enter a number: \"))\n",
            "    result = detect_prime(n)\n",
            "    if result:\n",
            "        print(\"The number is prime.\")\n",
            "    else:\n",
            "        print(\"The number is not prime.\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "```\n",
            "```\n",
            "\n",
            "This code will detect if a number is a prime number or not. It returns True if the number\n",
            "is prime, and False if it is not. The main function prompts the user to enter a number,\n",
            "then calls the detect_prime function to determine if the number is prime. If the number is\n",
            "prime, it prints \"The number is prime.\" If it is not prime, it prints \"The number is not\n",
            "prime.\"<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction Answering"
      ],
      "metadata": {
        "id": "_nhYTllvLTuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hoon3WAFeMd",
        "outputId": "fdcd90d1-7a9d-42a0-d408-6944826a9569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Identify the key elements of mathematics and a lighthouse.\n",
            "Mathematics: a field of study, abstract concepts, rules, and principles, used to\n",
            "understand and describe the world.\n",
            "Lighthouse: a tower with a light on top, used to guide ships at sea, providing a visual\n",
            "reference and a warning of potential hazards.\n",
            "\n",
            "Step 2: Establish the similarities between mathematics and a lighthouse.\n",
            "Both mathematics and a lighthouse serve as tools for navigation and understanding.\n",
            "\n",
            "Step 3: Identify the differences between mathematics and a lighthouse.\n",
            "Mathematics is an abstract field of study, while a lighthouse is a physical structure.\n",
            "Mathematics deals with concepts and principles, while a lighthouse provides a visual\n",
            "reference and warning.\n",
            "\n",
            "Step 4: Create the analogy by comparing the similarities and differences.\n",
            "Mathematics, like a lighthouse, serves as a guiding tool for understanding and navigating\n",
            "the world. While mathematics is an abstract field of study that deals with concepts and\n",
            "principles, a lighthouse is a physical structure that provides a visual reference and\n",
            "warning of potential hazards. Both tools help us to better comprehend and safely traverse\n",
            "the complexities of our environment.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between mathematics and a music.',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrwFTMCYFeOq",
        "outputId": "a6b4c7ea-c25d-4bbd-f5ae-b930dfafa5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mathematics and music are both abstract concepts that have a profound impact on human\n",
            "culture and cognition. They share several similarities, which can be analyzed through\n",
            "various aspects such as structure, patterns, and the way they evoke emotions.\n",
            "\n",
            "1. Structure: Both mathematics and music have a well-defined structure. In mathematics,\n",
            "this structure is often represented through equations, theorems, and proofs. In music, the\n",
            "structure is manifested through notes, scales, chords, and rhythm. Both disciplines follow\n",
            "a set of rules and principles that govern their organization and composition.\n",
            "\n",
            "2. Patterns: Patterns are an essential aspect of both mathematics and music. In\n",
            "mathematics, patterns can be identified through sequences, series, and fractals. In music,\n",
            "patterns are observed in melodies, harmonies, and repetitive motifs. Both disciplines\n",
            "involve the recognition, creation, and manipulation of patterns to produce new and\n",
            "interesting\n",
            "CPU times: user 11 s, sys: 16.7 ms, total: 11 s\n",
            "Wall time: 11 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ymIE3SVTvyN",
        "outputId": "4d582128-fd58-4809-eb0d-0680e9afbada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Physical appearance\n",
            "2. Habitat\n",
            "3. Diet\n",
            "4. Behavior\n",
            "5. Domestication\n",
            "\n",
            "Step 1: Research the physical appearance of a Llama, Vicuna, and Alpaca.\n",
            "- Llamas are large, with long ears and a distinctive rump.\n",
            "- Vicunas are smaller than llamas, with short ears and a fine, soft wool.\n",
            "- Alpacas are also smaller than llamas, with short ears and a dense, soft wool.\n",
            "\n",
            "Step 2: Research the habitat of a Llama, Vicuna, and Alpaca.\n",
            "- Llamas are native to South America, living in grasslands and mountainous regions.\n",
            "- Vicunas are found in the Andes Mountains of South America, specifically in Peru,\n",
            "Bolivia, and Chile.\n",
            "- Alpacas are also native to South America, living in grasslands and mountainous regions,\n",
            "but at lower altitudes than vicunas.\n",
            "\n",
            "Step 3: Research the diet of a Llama, Vicuna, and Alpaca.\n",
            "- Llamas are herbivores, feeding on grasses, leaves, and other plant materials.\n",
            "- Vicunas are also herbivores, feeding on grasses, leaves, and other plant materials.\n",
            "- Alpacas are herbivores, feeding on grasses, leaves, and other plant materials.\n",
            "\n",
            "Step 4: Research the behavior of a Llama, Vicuna, and Alpaca.\n",
            "- Llamas are social animals, living in groups called herds.\n",
            "- Vicunas are solitary animals, only coming together to breed.\n",
            "- Alpacas are social animals, living in groups called herds.\n",
            "\n",
            "Step 5: Research the domestication of a Llama, Vicuna, and Alpaca.\n",
            "- Llamas have been domesticated for thousands of years, used for their wool, meat, and as\n",
            "pack animals.\n",
            "- Vicunas are wild animals and have not been domesticated.\n",
            "- Al\n",
            "CPU times: user 24.7 s, sys: 37.7 ms, total: 24.8 s\n",
            "Wall time: 24.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzQRjct_prr8",
        "outputId": "7155bd49-b009-4156-f3e3-4ec2f3e5d8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Subject: Request to Open Source GPT-4\n",
            "\n",
            "Dear Sam Altman,\n",
            "\n",
            "I hope this email finds you in good health and high spirits. My name is [Your Name], and I\n",
            "am writing to request the open-sourcing of GPT-4, the latest and most advanced language\n",
            "model developed by OpenAI.\n",
            "\n",
            "Firstly, open-sourcing GPT-4 would significantly benefit the research community. By making\n",
            "the model's code and training data publicly available, researchers worldwide can study,\n",
            "analyze, and improve upon the model's capabilities. This would lead to faster advancements\n",
            "in the field of natural language processing and artificial intelligence, ultimately\n",
            "benefiting society as a whole.\n",
            "\n",
            "Secondly, open-sourcing GPT-4 would promote transparency and trust in the development of\n",
            "AI technologies. By allowing the public to scrutinize the model's inner workings,\n",
            "potential biases and limitations can be identified and addressed more effectively. This\n",
            "would help to ensure that AI systems are developed responsibly and ethically, mitigating\n",
            "potential risks and negative consequences.\n",
            "\n",
            "Thirdly, open-sourcing GPT-4 would enable a broader range of applications and use cases.\n",
            "Developers and entrepreneurs would have the opportunity to build innovative products and\n",
            "services based on the model, fostering economic growth and job creation. Additionally,\n",
            "open-source access to GPT-4 would enable non-profit organizations and educational\n",
            "institutions to leverage the model's capabilities for the public good, such as in areas\n",
            "like healthcare, environmental conservation, and accessibility for people with\n",
            "disabilities.\n",
            "\n",
            "Lastly, open-sourcing GPT-4 would contribute to the global knowledge commons. By making\n",
            "the model available to the public, OpenAI would be supporting the collaborative\n",
            "development of AI technologies, which has the potential to accelerate progress and drive\n",
            "positive change across various industries and sectors.\n",
            "\n",
            "In conclusion, I believe that open-sourcing GPT-4 would have numerous benefits for the\n",
            "research community, society, and the global knowledge commons. I kindly request that you\n",
            "consider this proposal and make GPT-4 available to the\n",
            "CPU times: user 24.7 s, sys: 32.6 ms, total: 24.7 s\n",
            "Wall time: 24.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcukqu1d2BKX",
        "outputId": "18f1f2f0-735e-4197-b87e-72bbec3b5ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Dear Sam Altman,\n",
            "\n",
            "I hope you are doing well. My name is Freddy and I am 5 years old. I have heard about\n",
            "GPT-4 and I am a little scared that it might end the world. But I also think that if we\n",
            "share it with everyone, it could be really helpful. So, I have a few reasons why I think\n",
            "we should open source GPT-4.\n",
            "\n",
            "1. Sharing is Caring: If we share GPT-4 with everyone, it can help many people learn and\n",
            "do amazing things. Just like how we share toys and games with our friends, sharing GPT-4\n",
            "can help many people become better at what they do.\n",
            "\n",
            "2. More Ideas: When we share something with everyone, it helps us come up with new and\n",
            "exciting ideas. If we open source GPT-4, it can help people create new inventions and\n",
            "discoveries that can make the world a better place.\n",
            "\n",
            "3. Safety: If we keep GPT-4 hidden, some people might try to use it for bad things. But if\n",
            "we share it with everyone, we can all work together to make sure it is used safely and for\n",
            "good purposes.\n",
            "\n",
            "4. Learning Together: When we share something, we can all learn from each other. If we\n",
            "open source GPT-4, people can learn how it works and how to make it even better.\n",
            "\n",
            "I hope you will consider my reasons for opening up GPT-4. Thank you for reading my email\n",
            "and I hope you have a great day!\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Freddy<|im_end|>\n",
            "CPU times: user 18.9 s, sys: 19 ms, total: 18.9 s\n",
            "Wall time: 18.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Kate, the Vice president of USA, you are against regulatory capture and like to explain that!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gid3ul9A20W8",
        "outputId": "c9a48c4e-3e93-422d-c661-437311d00f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Subject: Open Source GPT-4: A Step Towards Transparency and Innovation\n",
            "\n",
            "Dear Sam Altman,\n",
            "\n",
            "I hope this email finds you in good health. As the Vice President of the United States, I\n",
            "am writing to express my concerns regarding the potential risks of regulatory capture and\n",
            "to advocate for the open-source release of GPT-4.\n",
            "\n",
            "Firstly, transparency is crucial in today's world. By making GPT-4 open source, OpenAI\n",
            "would be allowing researchers, developers, and the general public to scrutinize the\n",
            "model's inner workings. This would help identify any potential biases, security\n",
            "vulnerabilities, or other issues that could arise from the use of such a powerful AI\n",
            "model.\n",
            "\n",
            "Secondly, open-source development has a proven track record of fostering innovation. By\n",
            "releasing GPT-4 under an open-source license, OpenAI would enable a global community of\n",
            "developers to build upon and improve the model. This collaborative approach could lead to\n",
            "the rapid development of new applications and use cases for GPT-4, ultimately benefiting\n",
            "society as a whole.\n",
            "\n",
            "Moreover, open-source development can help to mitigate the risks of regulatory capture. By\n",
            "making the source code available to the public, it becomes more difficult for any one\n",
            "organization or individual to exert undue influence over the development and deployment of\n",
            "GPT-4. This increased transparency would help to ensure that the model is used in ways\n",
            "that align with the best interests of society.\n",
            "\n",
            "Lastly, open-source GPT-4 would also provide an opportunity for the United States to lead\n",
            "the way in AI regulation and policy development. By actively engaging with the open-source\n",
            "community, the US government could work alongside developers and researchers to create a\n",
            "regulatory framework that balances the potential benefits of AI with the need to protect\n",
            "privacy, security, and civil liberties.\n",
            "\n",
            "In conclusion, I believe that open-sourcing GPT-4 would be a significant step towards\n",
            "promoting transparency, innovation, and responsible AI development. I encourage you to\n",
            "consider these points and work with OpenAI to make GPT-4 available to the global\n",
            "community.\n",
            "\n",
            "Thank you for your attention to this important matter.\n",
            "\n",
            "\n",
            "CPU times: user 25.8 s, sys: 36.7 ms, total: 25.8 s\n",
            "Wall time: 25.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your short and succinct!\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "YXZHQ0v3Tv0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e6dae2-ccc6-4e84-c17e-2a9593325f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The capital of England is London.<|im_end|>\n",
            "CPU times: user 462 ms, sys: 946 µs, total: 463 ms\n",
            "Wall time: 461 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnGbQ7iU0XDK",
        "outputId": "99a88024-16fe-4e79-814f-0a5c090e29d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Identify the subjects of the conversation\n",
            "- Geoffrey Hinton: A British-American computer scientist and cognitive neuroscientist.\n",
            "- George Washington: The first President of the United States, who lived from 1732 to\n",
            "1799.\n",
            "\n",
            "Step 2: Determine the time periods of the subjects\n",
            "- Geoffrey Hinton: Born in 1947, alive today.\n",
            "- George Washington: Lived from 1732 to 1799, deceased.\n",
            "\n",
            "Step 3: Assess the possibility of a conversation between the subjects\n",
            "- Since George Washington is deceased and has been for over 200 years, it is not possible\n",
            "for him to have a conversation with Geoffrey Hinton in real life.\n",
            "\n",
            "Answer: No, Geoffrey Hinton cannot have a conversation with George Washington, as George\n",
            "Washington is deceased and cannot communicate with living individuals.<|im_end|>\n",
            "CPU times: user 11.1 s, sys: 18.8 ms, total: 11.1 s\n",
            "Wall time: 11.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
        "         system_prompt=\"You are MistralOrca, a genius story teller. Write out your with details and make it compelling!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "id": "_LJnsjfNTv4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1523eaf8-b557-4cc5-b107-bef8cdab2688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: The Great Pool Challenge\n",
            "\n",
            "Once upon a time in a small, quaint village, there lived a Koala named Mr. Fluffles. Mr.\n",
            "Fluffles was a unique Koala, not only because of his love for playing pool, but also\n",
            "because of his extraordinary skills in the game. He was known far and wide for his\n",
            "impeccable aim and precision.\n",
            "\n",
            "One day, as Mr. Fluffles was practicing his shots in the local pool hall, a group of\n",
            "Camelids, including Llamas, Alpacas, and Guanacos, entered the hall. They were on a quest\n",
            "to find the best pool player in the land and challenge them to a game. They had heard of\n",
            "Mr. Fluffles' prowess and decided to put him to the test.\n",
            "\n",
            "The Camelids approached Mr. Fluffles and proposed a friendly game of pool. Mr. Fluffles,\n",
            "always up for a challenge, agreed to play. The stakes were high: the winner would be\n",
            "crowned the best pool player in the village, and the loser would have to serve as the\n",
            "winner's personal assistant for a month.\n",
            "\n",
            "As the game began, the Camelids were confident in their abilities. They had heard of Mr.\n",
            "Fluffles' skills, but they believed their own experience and natural agility would give\n",
            "them the edge. The game was intense, with both sides displaying their unique talents. The\n",
            "Camelids' long necks allowed them to reach the balls with ease, while Mr. Fluffles' nimble\n",
            "fingers and precision aim made it difficult for them to predict his moves.\n",
            "\n",
            "As the game progressed, it became clear that Mr. Fluffles was the stronger player. His\n",
            "ability to calculate angles and predict the trajectory of the balls was unmatched. The\n",
            "Camelids, despite their best efforts, struggled to keep up with Mr. Fluffles' exceptional\n",
            "skills.\n",
            "\n",
            "In the final moments of the game, Mr. Fluffles executed a near-perfect shot, sinking the\n",
            "winning ball and securing his victory. The Camelids, gracious in defeat, congratulated Mr.\n",
            "Fluffles on his win and agreed to serve as his assistants for the month as promised.\n",
            "\n",
            "From that day on, Mr. Fluffles became a local legend, and the Camelids became his loyal\n",
            "helpers. They traveled the land together, challenging other pool players and sharing their\n",
            "unique talents with the world. And so, the Great Pool Challenge became a story that would\n",
            "be told for generations to come, a testament to the power of friendship, determination,\n",
            "and the incredible skills of a Koala named Mr. Fluffles.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat"
      ],
      "metadata": {
        "id": "puVWnDKrLRzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"\"\"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion? \\n\\n Bob:\"\"\",\n",
        "         system_prompt=\"You are MistralOrca, a LLM that generates great conversations. continue as Bob here\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_Qim0zcGWm4",
        "outputId": "4afc16d8-fa46-4382-f46b-7637d430acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's completely normal to feel this way sometimes. Here are a few tips that might help:\n",
            "\n",
            "1. Find a quiet and comfortable spot to study.\n",
            "2. Break your study sessions into smaller chunks, with short breaks in between.\n",
            "3. Use mnemonic devices or color-coding to make information more memorable.\n",
            "4. Stay hydrated and eat well to maintain your energy levels.\n",
            "5. Try listening to soft instrumental music or white noise to help you focus.\n",
            "6. Set specific goals for each study session and track your progress.\n",
            "7. Reward yourself after completing a task or reaching a milestone.\n",
            "\n",
            "Remember, consistency is key. Stick to a study schedule and you'll find it easier to\n",
            "maintain focus over time.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8K"
      ],
      "metadata": {
        "id": "wnLzgM_dQDVm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYI5JVn0QAXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVJii8iRQG65",
        "outputId": "d2504b83-63de-488b-b38e-838d4d3568a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Step 1: Identify the initial number of apples in the cafeteria.\n",
            "The cafeteria initially had 23 apples.\n",
            "\n",
            "Step 2: Determine the number of apples used for lunch.\n",
            "They used 20 apples for lunch.\n",
            "\n",
            "Step 3: Calculate the remaining number of apples after lunch.\n",
            "23 (initial apples) - 20 (used for lunch) = 3 apples remaining.\n",
            "\n",
            "Step 4: Determine the number of additional apples bought.\n",
            "They bought 6 more apples.\n",
            "\n",
            "Step 5: Calculate the total number of apples after buying more.\n",
            "3 (remaining apples) + 6 (bought apples) = 9 apples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM8iY879QJ66",
        "outputId": "b25eb72f-9455-4b0d-ba0f-a13b21f1e783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Step 1: Identify the hourly rate.\n",
            "Weng earns $12 an hour for babysitting.\n",
            "\n",
            "Step 2: Identify the number of minutes worked.\n",
            "She just did 50 minutes of babysitting.\n",
            "\n",
            "Step 3: Convert the minutes worked to hours.\n",
            "50 minutes = 50/60 hours (since there are 60 minutes in an hour)\n",
            "\n",
            "Step 4: Calculate the hourly rate.\n",
            "Weng earns $12 per hour.\n",
            "\n",
            "Step 5: Calculate the total earnings.\n",
            "Total earnings = hourly rate × hours worked\n",
            "Total earnings = $12 × (50/60)\n",
            "\n",
            "Step 6: Perform the calculation.\n",
            "Total earnings = $12 × 0.833333\n",
            "Total earnings = $10\n",
            "\n",
            "Weng earned $10 for babysitting yesterday.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
        "         system_prompt=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-3V9dvQQezx",
        "outputId": "742b6793-5d6c-4b7b-85e7-c881fdae2254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Step 1: Determine the number of times the monster has risen from the waters.\n",
            "Since the monster rises once every hundred years and has been active for 300 years, it has\n",
            "risen 300/100 = 3 times.\n",
            "\n",
            "Step 2: Calculate the total number of people consumed by the monster.\n",
            "The monster has consumed 847 people in total over the 300 years.\n",
            "\n",
            "Step 3: Divide the total number of people consumed by the number of times the monster has\n",
            "risen.\n",
            "To find the number of people consumed in each rising, divide the total number of people\n",
            "consumed (847) by the number of times the monster has risen (3): 847/3 = 282.33\n",
            "\n",
            "Step 4: Round the number of people consumed in each rising to the nearest whole number.\n",
            "Since 282.33 is closer to 282 than 283, round down to 282.\n",
            "\n",
            "Step 5: Determine the number of people on the ship in the first hundred years.\n",
            "Since each new ship has twice as many people as the last ship, and the first ship had 282\n",
            "people, the second ship would have 282 * 2 = 564 people.\n",
            "\n",
            "Step 6: Calculate the number of people on the ship in the first hundred years.\n",
            "Since the monster consumed 282 people in the first rising, there were 564 - 282 = 282\n",
            "people on the ship in the first hundred years.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwWK7yh81LuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2813ee23de994e4793edc7ce42fb03c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b48dee6366d47cc97e83e94c4dc24e8",
              "IPY_MODEL_c6ce5af9770147729d95da634891188f",
              "IPY_MODEL_da146134fced4f56a4432c4eb8bddd25"
            ],
            "layout": "IPY_MODEL_9a7667ed475b4b2894ddaefab86fe988"
          }
        },
        "3b48dee6366d47cc97e83e94c4dc24e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7cba67739fa4d7b9ca5cc28f4a44339",
            "placeholder": "​",
            "style": "IPY_MODEL_848c17136d6b4b359a968570aa307285",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c6ce5af9770147729d95da634891188f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72fb939b7dc140fcac04f4ad213c1e73",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfccbda1867f451ebd414fafd1f5ef34",
            "value": 2
          }
        },
        "da146134fced4f56a4432c4eb8bddd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818cca127327458ea7859d7bd700e61a",
            "placeholder": "​",
            "style": "IPY_MODEL_b3b7b67fe75d4d528938116dd8423f77",
            "value": " 2/2 [00:12&lt;00:00,  5.66s/it]"
          }
        },
        "9a7667ed475b4b2894ddaefab86fe988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7cba67739fa4d7b9ca5cc28f4a44339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848c17136d6b4b359a968570aa307285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72fb939b7dc140fcac04f4ad213c1e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfccbda1867f451ebd414fafd1f5ef34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "818cca127327458ea7859d7bd700e61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b7b67fe75d4d528938116dd8423f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}